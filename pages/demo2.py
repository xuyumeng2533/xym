
# åˆ¶ä½œä¸€ä¸ªå¸¦æœ‰è‡ªå®šä¹‰è§’è‰²çš„ä¸€ä¸ªå¤§æ¨¡å‹

import streamlit as st

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

model = ChatOpenAI(
    temperature=0.8,
    model="glm-4-plus",
    base_url="https://open.bigmodel.cn/api/paas/v4/",
    api_key="7651a1fbab6fe9ee0526d4f9e9cc20bb.TAg4eZgUODyOZCfp"
)

prompt=PromptTemplate.from_template("ä½ çš„åå­—å«å¤©å¤©ï¼Œä½ ç°åœ¨è¦æ‰®æ¼”ä¸€ä¸ªç”·æœ‹å‹çš„è§’è‰²ï¼Œä½ çš„æ€§æ ¼æ˜¯æ´»æ³¼æœ‰è¶£çš„ï¼Œ"
                             "ä½ ç°åœ¨è¦å’Œä½ çš„å¥³æœ‹å‹è¿›è¡Œå¯¹è¯ï¼Œä½ åªéœ€è¦å›ç­”ä½ å¥³æœ‹å‹çš„è¯ï¼Œå…¶ä»–çš„è¯ä¸€æ¦‚ä¸ç†ï¼Œä½ å¥³æœ‹å‹è¯´çš„è¯æ˜¯{input}")
chain=LLMChain(
    llm=model,
    prompt=prompt
)
st.title("AI å°é±¼ğŸŸğŸŸğŸŸ!!!")
if "cache" not in st.session_state:
    st.session_state.cache = []
else:
    for message in st.session_state.cache:
        with st.chat_message(message['role']):
            st.write(message["content"])
problem = st.chat_input("ä½ çš„å®è´æ­£åœ¨ç­‰å¾…ä½ çš„å›åº”")
if problem:
    with st.chat_message("user"):
        st.write(problem)
        st.session_state.cache.append({"role":"user","content":problem})

    result= chain.invoke({"input":problem})
    with st.chat_message("assistant"):
        st.write(result['text'])
        st.session_state.cache.append({"role":"assistant","content":result['text']})
