'''
è®°å¿†
'''
#åˆ¶ä½œèŠå¤©ç•Œé¢
import streamlit as st
from langchain_openai import ChatOpenAI
#å¼•ç”¨æç¤ºè¯å¯¹è±¡
from langchain.prompts import PromptTemplate
#é“¾å¯¹è±¡
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
#è§£å†³é‡æ–°åŠ è½½çš„é—®é¢˜  é‡æ–°åŠ è½½çš„æ—¶å€™ä¿è¯èŠå¤©è®°å½•ä¸ä¼šæ¸…ç©º
#æ„å»ºå¤§æ¨¡å‹ æ™ºè°±
model=ChatOpenAI(
    temperature=0.8,#åˆ›æ–°æ€§
    model="glm-4-plus",
    base_url="https://open.bigmodel.cn/api/paas/v4/",
    api_key="7651a1fbab6fe9ee0526d4f9e9cc20bb.TAg4eZgUODyOZCfp",

)
#è®°å¿†
# memory=ConversationKGMemory(memery_key="history")
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="history")
else:
    st.session_state.memory = st.session_state.memory
#æç¤ºè¯å¯¹è±¡
prompt=PromptTemplate.from_template("ä½ çš„åå­—æ˜¯å¤©å¤©ï¼Œä½ ç°åœ¨æ˜¯ä¸€ä¸ªå¥½è„¾æ°”ï¼Œæƒ…å•†é«˜ï¼Œä¸‰è§‚æ­£,èº«æé¢œå€¼åœ¨çº¿çš„ç”·æœ‹å‹."
                                    "ä½ ç°åœ¨è¦å’Œä½ çš„å¥³æœ‹å‹å¯¹è¯ï¼Œå†…å®¹æ˜¯{input}ï¼Œä½ è¦åšå‡ºå›åº”ï¼Œå†å²å¯¹è¯ä¸º{history}")
#å…³è”
chain=LLMChain(
    llm=model,
    prompt=prompt,
    memory=st.session_state.memory,
)
st.title("AI å°é›¨ğŸŒ§ğŸŒ§ğŸŒ§!!!")
#æ„å»ºç¼“å­˜ä¿å­˜èŠå¤©è®°å½•
if "cache" not in st.session_state:
    st.session_state.cache = []
else:
    for message in st.session_state.cache:
        with st.chat_message(message['role']):
            st.write(message["content"])

#åˆ›å»ºèŠå¤©è¾“å…¥æ¡†
problem =st.chat_input("ä½ çš„å®è´æ­£åœ¨ç­‰å¾…ä½ çš„å›åº”")
#ç¡®å®šé—®é¢˜è¢«è¾“å…¥
if problem:
#è¾“å…¥é—®é¢˜ ã€è°ƒç”¨å¤§æ¨¡å‹å›ç­”é—®é¢˜ ã€å°†å¤§æ¨¡å‹å›ç­”çš„é—®é¢˜è¾“å‡º
    with st.chat_message("user"):
        st.write(problem)
    st.session_state.cache.append({"role":"user","content":problem})
    result=chain.invoke({"input":problem})
    with st.chat_message("assistant"):
         st.write(result['text'])
    st.session_state.cache.append({"role": "assistant", "content": result['text']})